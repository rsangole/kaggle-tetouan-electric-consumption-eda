---
title: "Tétouan Electric Consumption - EDA & Predictive Modeling in R - Part 1"
author: "R Sangole"
date: "Aug 4, 2022"
output: 
  html_document: 
    toc: yes
    highlight: kate
    theme: paper
    code_folding: hide
    fig_width: 12
    fig_height: 4.5
    number_sections: yes
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, error=FALSE)
knitr::opts_chunk$set(out.width="100%", fig.height = 6, split=FALSE, fig.align = 'default')
options(dplyr.summarise.inform = FALSE)
```

![](https://storage.googleapis.com/kaggle-datasets-images/2380926/4017301/f43904962439a588d97e0069768f54a2/dataset-cover.jpg?t=2022-08-01-21-28-32){width="654"}

# Introduction

Welcome to the EDA and some time-series modeling of the the [Tétouan Electrical Consumption data](https://www.kaggle.com/datasets/fedesoriano/electric-power-consumption), uploaded by [fedesoriano](https://www.kaggle.com/fedesoriano).

In this notebook, you'll see,

-   Traditional time-series exploratory visuals, to get an understanding of the dataset
-   Multivariate PCA exploration, to further exploring the dataset
-   Time Series Linear Modeling to create a predictive linear model for power consumption

Tétouan is a city in northern Morocco with a population of \~0.5 million. Look at these gorgeous photos from [Wikipedia](https://en.wikipedia.org/wiki/Tétouan).

|                                                                                                                                      |                                                                                                                 |
|:--------------------------:|:-----------------------------:|
| ![](https://upload.wikimedia.org/wikipedia/commons/a/a0/A_view_of_Bouanane%252C_a_popular_destination_in_Tetouan._.jpg){width="355"} | ![](https://upload.wikimedia.org/wikipedia/commons/f/f7/A_view_of_mount_ghorghiz_in_Tetouan._.jpg){width="394"} |

# Initial Setup {.tabset}

*Read through the initial setup in the 4 tabs below.*

## Libraries {.tabset}

First, some I import some useful libraries and set some plotting defaults. I have to install two packages here, which are currently not in the Kaggle-R docker container.

```{r libraries, message=FALSE, warning=FALSE}
# Data Manipulation
library(dplyr)
library(tidyr)
library(readr)
library(skimr)
library(purrr)
library(stringr)
library(urltools)
library(magrittr)
library(lubridate)

# Plots
library(ggplot2)
library(naniar)
library(packcircles)
library(ggridges)
library(ggbeeswarm)
library(patchwork)

# PCA
if(!require(FactoMineR))
  remotes::install_cran("FactoMineR")
if(!require(factoextra))
  remotes::install_cran("factoextra")
library(FactoMineR)
library(factoextra)

# Tables
library(reactable)

# Settings
theme_set(theme_minimal(
  base_size = 13,
  base_family = "Menlo"))
theme_update(
  plot.title.position = "plot"
)
```

## Read In {.tabset}

Let's start be reading in the data. There is only one CSV file. `{janitor::clean_names}` helps us get clean column names quickly. Here's a peek into the dataset.

```{r message=FALSE, warning=FALSE}
dat <- read_csv("../input/electric-power-consumption/powerconsumption.csv") %>% 
  janitor::clean_names()
glimpse(dat, 100)
```

## Quick View {.tabset}

[`{skimr}`](https://docs.ropensci.org/skimr/index.html) gives a detailed dive into the dataset. Few takeaways:

- `datetime` is imported as character, and needs to be handled
- no missing data at all (thank you, fedesoriano)
- zone 1, 2 and 3 are in the order of their mean and p90 power consumption ranges
- finally, I'll be honest - I have no idea what `general diffuse flows` or `diffuse flows` mean, despite the definition in the data page

```{r paged.print=FALSE}
skimr::skim(dat)
```

# Interesting Questions

Since this is an open ended exploration, I will posit some questions which will guide the flow of further work.

1.  How does the power consumption vary by each zone?
2.  Are there temporal patterns in the power consumption?
3.  What's the relationship with temperature, humidity, etc?
4.  What does a multivariate study of the data set show?
5.  Are there customer usage patterns found in the time series?
6.  Can a linear predictive model be built using simple features?

# Feature Development

To aid answering many of these, I first need to create a few new features in the data set.

*We go from 9 columns to 24 columns in the data set.*

## Date-Time Features

Date-time features are excellent ways to add more depth to the time dimension of the dataset. The `timetk::tk_augment_timeseries_signature` is one of the best tools to do so. The function creates a bunch of columns I don't want for this analysis, so I exclude them. Finally, I'm renaming the power for brevity's sake.

```{r}
dat <- dat %>% 
  mutate(datetime = lubridate::as_datetime(datetime, format = "%m/%d/%Y %H:%M"),
         dt_hr = lubridate::round_date(datetime, "hour")) %>%
  timetk::tk_augment_timeseries_signature(datetime) %>%
  select(
    - matches("(xts)|(second)|(minute)|(iso)|(num)|(diff$)|(hour12)|(am.pm)|(week\\d)|(mday7)")
  ) %>% 
  mutate(hour = factor(hour, ordered = TRUE)) %>% 
  rename_with(~str_replace(.x, "power_consumption_", ""), contains("zone"))

dat %>% glimpse(80)
```

# Graphical EDA

There are xx components for the EDA below.

1. 
1. 

## TS Plot {.tabset}

```{r}
dat %>% 
  select(datetime, contains("zone")) %>% 
  tidyr::pivot_longer(-datetime) %>% 
  ggplot(aes(datetime, value)) +
  geom_vline(xintercept = as.POSIXct("2017-06-26"), color = "black", lty = 2) +
  geom_line(aes(color = name)) +
  labs(
    x = NULL,
    y = "Power Consumption",
    title = "Year of 2017",
    color = "Zone"
  ) -> p1

dat %>% 
  ggplot(aes(datetime, temperature)) +
  geom_line() +
  labs(
    x = NULL,
    y = "Temperature"
  ) -> p2

dat %>% 
  ggplot(aes(datetime, humidity)) +
  geom_line() +
  labs(
    x = NULL,
    y = "Humidity"
  ) -> p3

dat %>% 
  ggplot(aes(datetime, wind_speed)) +
  geom_line() +
  labs(
    x = NULL,
    y = "Wind Speed"
  ) -> p4

dat %>% 
  select(datetime, contains("flows")) %>% 
  tidyr::pivot_longer(-datetime) %>% 
  ggplot(aes(datetime, value)) +
  geom_line(aes(color = name)) +
  labs(
    x = NULL,
    y = "Flows",
    color = "Flows"
  ) -> p42

dat %>% 
  filter(datetime > "2017-06-16", datetime < "2017-07-06") %>% 
  select(datetime, contains("zone")) %>% 
  tidyr::pivot_longer(-datetime) %>% 
  ggplot(aes(datetime, value)) +
  geom_vline(xintercept = as.POSIXct("2017-06-26"), color = "black", lty = 2) +
  geom_line(aes(color = name)) +
  labs(
    x = NULL,
    y = "Temperature",
    title = "Zoomed: 15 Dec to 31 Dec 2017",
    color = "Zone"
  )-> p6
p1 / p2 / p3 / p4 / p42 / p6
```

```{r}
split_date <- as.Date("2017-06-27")
n_days <- 7
dat %>% 
  filter(datetime > split_date - n_days, datetime < split_date + n_days) %>% 
  mutate(split = ifelse(datetime > split_date, "After", "Before")) %>% 
  select(split, contains("zone")) %>% 
  tidyr::pivot_longer(-split) %>% 
  ggplot(aes(x = paste(name, split), y = value, color = split)) + 
  geom_boxplot() +
  geom_jitter(width = 0.2, alpha = 0.3)
```

## Seasonalities {.tabset}

```{r}
dat %>% 
  select(wday.lbl, contains("zone")) %>% 
  tidyr::pivot_longer(-wday.lbl) %>% 
  ggplot(aes(wday.lbl, value)) +
  geom_boxplot(aes(color = name))
```

```{r}
dat %>% 
  select(hour, contains("zone")) %>% 
  tidyr::pivot_longer(-hour) %>% 
  ggplot(aes(hour, value)) +
  geom_boxplot(aes(color = name))
```

```{r}
dat %>% 
  select(month.lbl, contains("zone")) %>% 
  tidyr::pivot_longer(-month.lbl) %>% 
  ggplot(aes(month.lbl, value)) +
  geom_boxplot(aes(color = name))
```

```{r}
dat %>% 
  select(datetime, contains("zone")) %>% 
  tidyr::pivot_longer(-datetime) %>% 
  group_by(name) %>% 
  timetk::plot_time_series_boxplot(datetime, value, "1 month")
dat %>% 
  select(datetime, month.lbl, contains("zone")) %>% 
  tidyr::pivot_longer(-datetime:-month.lbl) %>% 
  group_by(name) %>% 
  timetk::plot_time_series_boxplot(datetime, value, "1 week")
```

```{r}
dat %>% 
  timetk::plot_time_series_regression(
    datetime,
    zone1 ~ month.lbl + temperature + humidity + wind_speed + as.factor(day) + hour + week + lag(zone1) + as.factor(datetime>split_date),
    .show_summary = TRUE
  )
dat %>% 
  timetk::plot_time_series_regression(
    datetime,
    zone2 ~ month.lbl + temperature + humidity + wind_speed + as.factor(day) + hour + week + lag(zone2) + as.factor(datetime>split_date),
    .show_summary = TRUE
  )

dat %>% 
  timetk::plot_time_series_regression(
    datetime,
    zone3 ~ month.lbl + temperature + humidity + wind_speed + as.factor(day) + hour + week + lag(zone3) + as.factor(datetime>split_date),
    .show_summary = TRUE
  )
```

```{r}
dat %>% 
  timetk::plot_seasonal_diagnostics(datetime, zone1)
```

# `{r} # dat %>%  #   group_by(date = as.Date(dt_hr)) %>%  #   summarise(mean_zone1 = mean(zone1)) %>%  #   timetk::plot_stl_diagnostics(date, mean_zone1) #`

## Principal Component Analysis {.tabset}

One of my favorite methods of looking at multivariate numerical data is the simple, yet powerful tool - PCA. It's a great way to extract layers upon layers of information about the dataset from just a few plots. Let's explore this dataset using PCA.

At a high-level, here is the workflow:

1.  Calculate the principal components
2.  Explore the variables on a correlation circle
3.  Explore the individual observations on a biplot

We can throw in some clustering and data manipulation to get a pretty rich understanding of these data.

### Calc Components {.tabset}

Here, I'm calculating the principal components on a scaled data set of the numeric features. I'm imputing the missing values of sodium to the median value. `prcomp()` is a popular method, but I like the `FactoMineR::PCA()` method which offers some great features for post processing.

```{r message=FALSE, warning=FALSE}
dat %>% 
  select(temperature, humidity, month, day, hour, wday) %>%
  mutate(hour = as.numeric(hour)) %>% 
  scale() %>% 
  as.data.frame() -> dat_scaled
pca <- PCA(dat_scaled, graph = FALSE)
pca
```

We can see that the 1st two PCs account for \~70% of variation in the numerical data. Not too shabby!

```{r}
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 50))
```

### Explore Variables {.tabset}

The 'variable correlation plot' or 'correlation circle plot' is an insightful plot. It shows the relationships between all the features. Summarizing from this [article](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/#visualization-and-interpretation):

-   Positively correlated variables are grouped together.
-   Negatively correlated variables are positioned on opposite sides of the origin.
-   Distance between variables and the origin measures the quality of the variables, variables that are away from the origin are well represented.

What are the takeaways?

1.  We can see 6 distinct clusters of features, which I've highlighted in the plot after running kmeans on the coordinates of the two principal component loadings of the features.
    -   Sugars and serving size are highly correlated
    -   Carb is on it's own
    -   Calories and Saturated Fat are correlated
    -   As are the Total Fat, Sodium and Proteins
    -   Cholesterol and energy density are correlated
2.  Almost all the features (except trans fat, and perhaps cholesterol) are heavily loaded in the 1st two PCs (i.e., they are close to the correlation circle), which means they're all actively participating in these components.
3.  Proteins, total fat, and sodium are slightly negatively correlated with features like total sugars and added sugars.

```{r}
clusts <- kmeans(pca$var$coord[,1:2], 5, nstart = 50)
fviz_pca_var(
  pca,
  repel = TRUE,
  col.var = as.factor(clusts$cluster),
  legend.title = "Cluster"
) -> p1

clusts <- kmeans(pca$var$coord[,2:3], 5, nstart = 50)
fviz_pca_var(
  pca,
  axes = 2:3,
  repel = TRUE,
  col.var = as.factor(clusts$cluster),
  legend.title = "Cluster"
) -> p2

dat_scaled %>% 
  cor() %>% 
  ggcorrplot::ggcorrplot(method = "circle", hc.order = TRUE) -> p3

p1|p2|p3
```

### Explore Individuals {.tabset}

Now that we've looked at the features, let's look at the individual data points on the 1st two PCs.

First, we plot just the points across PC1 and PC2. Immediately, we can see distinct clusters for each of the Menu types. On the left, the drinks form an unusually straight line parallel to PC2 - notice how the smalls are towards the x-axis, and large drinks on top. McCafe menu items take the center-half of the plot. Condiments occupy the 3rd quadrant, while the regular and gourmet menu are spread across and 1st and 4th quadrant.

```{r message=FALSE, warning=FALSE}
fviz_pca_ind(pca,
             # col.ind = dat$,
             geom = "point",
             alpha = 0.2,
             select.ind = list(cos2 = 0.6)) +
    coord_equal() -> p1
fviz_pca_ind(pca,
             axes = 2:3,
             geom = "point",
             alpha = 0.2,
             select.ind = list(cos2 = 0.6))+
    coord_equal() -> p2
p1 | p2
```

Now, if we superimpose the feature loadings on top of the plot above, we can extract quite a few insights.

-   beverages certainly follow the sugars vectors from small to large
-   'Chicken Cheese Lava Burger' and 'Veg Maharaja Mac' have the highest calorie counts
-   The chicken burgers have the highest protein content, as expected
-   The McCafe menus have moderate sugars but also moderate carbs

*Remember, the axes center (0,0) indicate the region of average values for the principal component loadings. i.e., individual data points close to the axis will tend to have values close to the average of the dataset.*

```{r message=FALSE, warning=FALSE}
biplots <- function(pca,
                    axes,
                    color_by,
                    legend_title,
                    alpha = 0.2,
                    cos2_min = 0.6) {
  fviz_pca_biplot(
    pca,
    axes = axes,
    geom.ind = "point",
    col.ind = color_by,
    alpha.var = "cos2",
    select.ind = list(cos2 = cos2_min)
  ) +
    coord_equal() +
    labs(color = legend_title,
         shape = legend_title,
         alpha = "Feature Quality")
}


biplots(pca, 1:2, cut(dat$zone1, 5), "Zone 1") -> p1
biplots(pca, 2:3, cut(dat$zone1, 5), "Zone 1") -> p2
p1 | p2


biplots(pca, 1:2, cut(dat$zone1, 5), "Zone 1") -> p1
biplots(pca, 1:2, cut(dat$zone2, 5), "Zone 2") -> p2
biplots(pca, 1:2, cut(dat$zone3, 5),"Zone 3") -> p3
biplots(pca, 1:2, dat$month.lbl, "Month") -> p4
(p1|p2)/(p3|p4)



biplots(pca, 1:2, cut(dat$zone1, 5), "Zone 1") -> p1
biplots(pca, 2:3, cut(dat$zone1, 5), "Zone 1") -> p2
p1 | p2


biplots(pca, 1:2, cut(dat$zone2, 5), "Zone 2") -> p1
biplots(pca, 2:3, cut(dat$zone2, 5), "Zone 2") -> p2
p1 | p2


```

We can roughly call Principal Component 1 (Dim1) as the 'fats, proteins and calories' axis, while Principal Component 2 (Dim2) is the 'sugars and carbs' axis. Another way to look at the two dims is using a factor contribution plot, like the one below.

```{r}
p1 <- fviz_contrib(pca, choice = "var", axes = 1, top = 10)
p2 <- fviz_contrib(pca, choice = "var", axes = 2, top = 10)
p3 <- fviz_contrib(pca, choice = "var", axes = 3, top = 10)
p4 <- fviz_contrib(pca, choice = "var", axes = 4, top = 10)
p1/p2/p3/p4
```

------------------------------------------------------------------------

That was a fun exploration of these data. What else can you think of to explore?

Cheers!
